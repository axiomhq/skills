#!/bin/bash
# Analyze query coverage to score and rank unused data opportunities
#
# Usage: analyze-query-coverage <deployment> <dataset> [field] [query_days] [ingest_days]
#
# Outputs ranked opportunities with scores based on:
#   - Ingest volume (high = more savings potential)
#   - Query frequency (low = safer to drop)
#   - Coverage ratio (% of queries that use this data)

set -euo pipefail

AXIOM_API="${AXIOM_API:-$HOME/.config/agents/skills/axiom-sre/scripts/axiom-api}"
AXIOM_QUERY="${AXIOM_QUERY:-$HOME/.config/agents/skills/axiom-sre/scripts/axiom-query}"
DEPLOYMENT="${1:-}"
DATASET="${2:-}"
FIELD="${3:-}"
QUERY_DAYS="${4:-30}"    # Query history lookback (axiom-history, cheap)
INGEST_DAYS="${5:-1}"    # Ingest volume lookback (actual dataset, expensive)

if [[ -z "$DEPLOYMENT" || -z "$DATASET" ]]; then
    cat << 'EOF'
Usage: analyze-query-coverage <deployment> <dataset> [field] [query_days] [ingest_days]

Scores data by query coverage to find optimization opportunities.

Arguments:
  deployment   Axiom deployment name
  dataset      Dataset to analyze
  field        (optional) Specific field to analyze values for
  query_days   (optional) Query history lookback, default 30 (cheap)
  ingest_days  (optional) Ingest volume lookback, default 1 (expensive)

Without field: Ranks columns by query frequency (find rarely-used columns)
With field:    Ranks field values by query coverage (find rarely-queried subsets)

Examples:
  analyze-query-coverage prod k8s-logs              # Rank columns (30d queries, 1d ingest)
  analyze-query-coverage prod k8s-logs app          # Rank app values
  analyze-query-coverage prod k8s-logs app 90 7     # 90d query history, 7d ingest volume
EOF
    exit 1
fi

[[ -x "$AXIOM_API" ]] || { echo "Error: axiom-api not found" >&2; exit 1; }
[[ -x "$AXIOM_QUERY" ]] || { echo "Error: axiom-query not found" >&2; exit 1; }
command -v jq >/dev/null 2>&1 || { echo "Error: jq required" >&2; exit 1; }

echo "=== Query Coverage Analysis ==="
echo "Deployment: $DEPLOYMENT"
echo "Dataset: $DATASET"
echo ""

# Preflight: Use axiom-audit for volume estimate (no scan of actual dataset)
echo "--- Preflight: Checking volume via axiom-audit ---"
VOLUME_INFO=$($AXIOM_QUERY "$DEPLOYMENT" "
['axiom-audit']
| where action == 'usageCalculated'
| where ['properties.dataset'] == '$DATASET'
| where _time > ago(24h)
| summarize 
    total_bytes = sum(toreal(['properties.hourly_ingest_bytes'])),
    hours = dcount(bin(_time, 1h))
| extend gb_per_day = round(total_bytes / hours * 24 / 1000000000, 1)
| project gb_per_day
" 2>&1 | grep -E "^gb_per_day=" || echo "gb_per_day=0")

GB_PER_DAY=$(echo "$VOLUME_INFO" | sed 's/.*gb_per_day=\([^ ]*\).*/\1/')

echo "Dataset: ~${GB_PER_DAY} GB/day"

# Estimate scan size for ingest query
if [[ "$GB_PER_DAY" != "0" ]]; then
    ESTIMATED_GB=$(echo "$GB_PER_DAY * $INGEST_DAYS" | bc 2>/dev/null || echo "?")
    echo "Ingest query will scan: ~${ESTIMATED_GB} GB (${INGEST_DAYS}d)"
    
    # Warn if scan is large (>500 GB)
    if [[ "$ESTIMATED_GB" != "?" ]] && (( $(echo "$ESTIMATED_GB > 500" | bc -l 2>/dev/null || echo 0) )); then
        echo "âš ï¸  Large scan! Consider reducing ingest_days"
    fi
else
    echo "No volume data in axiom-audit - dataset may be new or name incorrect"
fi

echo "Query history: ${QUERY_DAYS}d (axiom-history, cheap)"
echo ""

# Create jq filter for AST traversal
JQ_FILTER=$(mktemp)
trap "rm -f $JQ_FILTER" EXIT

cat > "$JQ_FILTER" << 'JQEOF'
# Recursively extract all column/field references from AST
def extract_columns:
  if type == "array" then .[] | extract_columns
  elif type == "object" then
    (if .kind == "Entity" and .name then .name else empty end),
    (to_entries[] | .value | extract_columns)
  else empty
  end;

# Extract filter predicates with field/op/value
def extract_predicates:
  if type != "object" then empty
  elif .kind == "BinaryExpr" and (.op | IN("==", "!=", "contains", "!contains", "startswith", "endswith")) then
    { field: (.left.name // null), op: .op, value: (.right.value // null) }
  elif .kind == "BinaryExpr" and (.op | IN("and", "or")) then
    (.left | extract_predicates), (.right | extract_predicates)
  elif .kind == "InExpr" then
    { field: (.left.name // null), op: "in", values: [.right.list[]?.value] }
  elif .kind == "CallExpr" and .func.name == "not" then
    .params[]?.expr | extract_predicates
  else empty
  end;

# Main extraction
{
  all_columns: [.body | extract_columns] | unique,
  where_predicates: [.body.operations[]? | select(.kind == "Where") | .predicate | extract_predicates],
  summarize_groups: [.body.operations[]? | select(.kind == "Summarize") | .groups[]?.expr.name // .groups[]?.aliases[0]] | map(select(. != null)),
  has_wildcard: ([.body.operations[]? | select(.kind == "Project") | .fields[]? | select(.kind == "Entity" and .name == "*")] | length > 0)
}
JQEOF

# Fetch and parse queries
echo "Fetching query history from axiom-history..."
PARSED_QUERIES=$($AXIOM_API "$DEPLOYMENT" POST "/v1/datasets/_apl?format=tabular" "{
  \"apl\": \"['axiom-history'] | where _time > ago(${QUERY_DAYS}d) | where kind == 'apl' | where dataset == '$DATASET' | extend parsed = parse_apl(['query.apl']) | project parsed\"
}" 2>&1)

QUERY_COUNT=$(echo "$PARSED_QUERIES" | jq -r '.tables[0].columns | length // 0')
echo "Found $QUERY_COUNT queries to analyze"

if [[ "$QUERY_COUNT" -eq 0 ]]; then
    echo ""
    echo "No queries found. Either:"
    echo "  - Dataset '$DATASET' hasn't been queried in ${QUERY_DAYS} days"
    echo "  - Dataset name is incorrect"
    echo ""
    echo "This means ALL data in this dataset is potentially unused!"
    exit 0
fi

# Extract structured info from each query
echo "Parsing AST and extracting usage patterns..."
QUERY_SUMMARIES=$(echo "$PARSED_QUERIES" | jq -r '.tables[0].columns[][0]' | jq -f "$JQ_FILTER" -c 2>/dev/null | jq -s '.')

# Aggregate column usage across all queries
COLUMN_USAGE=$(echo "$QUERY_SUMMARIES" | jq '
  . as $queries |
  ($queries | length) as $total |
  [
    $queries[].all_columns[]
  ] | group_by(.) | map({
    column: .[0],
    query_count: length,
    coverage_pct: (100 * length / $total | floor)
  }) | sort_by(-.query_count)
')

WILDCARD_COUNT=$(echo "$QUERY_SUMMARIES" | jq '[.[] | select(.has_wildcard)] | length')

# If no field specified, show column usage ranking
if [[ -z "$FIELD" ]]; then
    echo ""
    echo "--- Column Usage Ranking ---"
    echo "Total queries: $QUERY_COUNT | Queries with wildcards: $WILDCARD_COUNT"
    echo ""
    echo "$COLUMN_USAGE" | jq -r '
      .[:30][] | 
      "\(.column)\tqueries=\(.query_count)\tcoverage=\(.coverage_pct)%"
    ' | column -t -s$'\t'
    
    # Find columns that are never referenced
    echo ""
    echo "--- Columns Never Referenced in Queries ---"
    echo "(Compare against dataset schema to find unused columns)"
    echo ""
    echo "To get dataset schema:"
    echo "  axiom-query $DEPLOYMENT \"['$DATASET'] | take 1 | getschema\""
    echo ""
    echo "To analyze a specific field's values:"
    echo "  $0 $DEPLOYMENT $DATASET <field_name>"
    exit 0
fi

echo "Analyzing field: $FIELD"
echo ""

# For specific field analysis: check how queries reference this field
FIELD_ANALYSIS=$(echo "$QUERY_SUMMARIES" | jq --arg f "$FIELD" '
  . as $queries |
  ($queries | length) as $total |
  
  # Queries that reference this field anywhere (in any column list)
  [$queries[] | select(.all_columns | index($f))] as $refs |
  ($refs | length) as $references_field |
  
  # Queries that have a where predicate on this field
  [$queries[] | select([.where_predicates[]? | select(.field == $f)] | length > 0)] as $filtered |
  ($filtered | length) as $filters_with_field |
  
  # Queries that group by this field
  [$queries[] | select(.summarize_groups | index($f))] as $grouped |
  ($grouped | length) as $groups_by_field |
  
  # Queries that reference field but dont have specific filter predicates
  # (means ALL values of this field are potentially accessed)
  ([$refs[] | select([.where_predicates[]? | select(.field == $f)] | length == 0)] | length) as $unfiltered_refs |
  
  # Extract all filtered values across all queries
  [
    $queries[].where_predicates[]? | 
    select(.field == $f) |
    if .values then .values[] else .value end
  ] | map(select(. != null)) | unique as $filtered_values |
  
  {
    total_queries: $total,
    references_field: $references_field,
    queries_with_filter: $filters_with_field,
    groups_by_field: $groups_by_field,
    unfiltered_references: $unfiltered_refs,
    filtered_values: $filtered_values,
    distinct_values_filtered: ($filtered_values | length),
    field_coverage_pct: (if $total > 0 then (100 * $references_field / $total | floor) else 0 end)
  }
')

echo "--- Query Pattern Analysis for '$FIELD' ---"
echo "$FIELD_ANALYSIS" | jq -r '
  "Total queries:              \(.total_queries)",
  "Queries referencing field:  \(.references_field) (\(.field_coverage_pct)%)",
  "  - With value filters:     \(.queries_with_filter) queries (\(.distinct_values_filtered) distinct values)",
  "  - Group by (summarize):   \(.groups_by_field)", 
  "  - No filter (all values): \(.unfiltered_references)"
'

UNFILTERED=$(echo "$FIELD_ANALYSIS" | jq '.unfiltered_references')
FILTERED_VALUES=$(echo "$FIELD_ANALYSIS" | jq -r '.filtered_values[]' 2>/dev/null | sort -u)
FILTERED_COUNT=$(echo "$FILTERED_VALUES" | grep -c . 2>/dev/null || echo 0)

echo ""

if [[ "$UNFILTERED" -gt 0 ]]; then
    echo "âš ï¸  WARNING: $UNFILTERED queries reference '$FIELD' without filtering."
    echo "   This means ALL values of this field are technically 'used'."
    echo "   However, these may be exploratory queries. Review before deciding."
    echo ""
fi

if [[ "$FILTERED_COUNT" -gt 0 ]]; then
    echo "--- Values explicitly filtered in queries ---"
    echo "$FILTERED_VALUES" | head -30
    if [[ "$FILTERED_COUNT" -gt 30 ]]; then
        echo "... and $((FILTERED_COUNT - 30)) more"
    fi
    echo ""
fi

# Get top values by volume from dataset (use same time range as query history)
echo "--- Top Values by Ingest Volume (last ${INGEST_DAYS}d, sampled) ---"
$AXIOM_QUERY "$DEPLOYMENT" "
['$DATASET']
| where _time > ago(${INGEST_DAYS}d)
| sample 0.001
| summarize sampled = count() by value = tostring(['$FIELD'])
| extend est_events = sampled * 1000
| order by est_events desc
| take 20
| project ['$FIELD'] = value, est_events
"

echo ""
echo "--- Recommendations ---"
if [[ "$UNFILTERED" -eq 0 && "$FILTERED_COUNT" -gt 0 ]]; then
    echo "âœ… All queries that reference '$FIELD' use specific filters."
    echo "   Values NOT in the filter list above are candidates for dropping."
    echo ""
    echo "   To find high-volume values not in filter list, cross-reference:"
    echo "   - Top values by volume (above)"
    echo "   - Values in query filters"
else
    echo "ðŸ“Š $UNFILTERED queries reference '$FIELD' without filtering."
    echo "   This doesn't mean you can't optimize - review those queries:"
    echo "   - Are they exploratory/ad-hoc? (may be acceptable to ignore)"
    echo "   - Are they dashboards/alerts? (need the data)"
    echo ""
    echo "   Consider: even if some queries are unfiltered, high-volume values"
    echo "   that are RARELY queried are still optimization candidates."
fi
