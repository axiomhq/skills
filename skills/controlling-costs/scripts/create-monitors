#!/bin/bash
# Create 4 cost control monitors using hybrid threshold + statistical approach
#
# Monitors:
#   1. Budget Guardrail (threshold) - immediate reaction to overspend
#   2. Per-Dataset Spike (robust z-score) - statistical attribution of which dataset changed
#   3. Query Cost Spike (spotlight) - statistical detection of query cost changes
#   4. Reduction Glidepath (threshold) - progress tracking toward target
#
# Usage: create-monitors -d <deployment> -a <audit-dataset> -c <contract-bytes> [options]

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "$0")" && pwd)"
AXIOM_API="${AXIOM_API:-$HOME/.config/agents/skills/sre/scripts/axiom-api}"

# Source shared utilities
source "$SCRIPT_DIR/lib/format-bytes.sh"

# Defaults
DEPLOYMENT=""
AUDIT_DATASET=""
NOTIFIER_ID=""
CONTRACT_BYTES=""
GLIDEPATH_BYTES=""
FORCE=false

# Robust z-score thresholds (used for both ingest and query cost spike detection)
ZSCORE_THRESHOLD="3"            # Standard anomaly threshold (~0.1% false positive rate)
SPIKE_HOURS_THRESHOLD="2"       # Require sustained anomaly (not transient)

usage() {
    cat << 'EOF'
Usage: create-monitors -d <deployment> -a <audit-dataset> -c <contract> [options]

Create 4 hybrid cost control monitors.

Required:
  -d, --deployment NAME    Axiom deployment name
  -a, --audit-dataset NAME Audit dataset name (e.g., axiom-audit)
  -c, --contract BYTES     Daily contract limit in bytes (e.g., 167000000000000)
                           Also accepts units: 167TB, 5PB, 500GB

Optional:
  -n, --notifier ID        Notifier ID for alerts (omit for no alerts)
  -g, --glidepath BYTES    Initial glidepath target (default: contract × 1.5)
  --force                  Delete existing monitors and recreate

Example:
  create-monitors -d prod -a axiom-audit -c 167TB -n "abc123"
  create-monitors -d dev -a axiom-audit -c 500000000000  # 500 GB in bytes
EOF
    exit 1
}

# Parse arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        -d|--deployment)
            DEPLOYMENT="$2"
            shift 2
            ;;
        -a|--audit-dataset)
            AUDIT_DATASET="$2"
            shift 2
            ;;
        -n|--notifier)
            NOTIFIER_ID="$2"
            shift 2
            ;;
        -c|--contract)
            CONTRACT_BYTES=$(parse_bytes "$2")
            shift 2
            ;;
        -g|--glidepath)
            GLIDEPATH_BYTES=$(parse_bytes "$2")
            shift 2
            ;;
        --force)
            FORCE=true
            shift
            ;;
        -h|--help)
            usage
            ;;
        *)
            echo "Error: Unknown option $1" >&2
            usage
            ;;
    esac
done

# Validate required args
[[ -z "$DEPLOYMENT" ]] && { echo "Error: -d/--deployment required" >&2; usage; }
[[ -z "$AUDIT_DATASET" ]] && { echo "Error: -a/--audit-dataset required" >&2; usage; }
[[ -z "$CONTRACT_BYTES" || "$CONTRACT_BYTES" == "0" ]] && { echo "Error: -c/--contract required" >&2; usage; }

# Validate dependencies
command -v jq >/dev/null 2>&1 || { echo "Error: jq required" >&2; exit 1; }
[[ -x "$AXIOM_API" ]] || { echo "Error: axiom-api not found at $AXIOM_API" >&2; exit 1; }

# Default glidepath to 1.5x contract if not specified
if [[ -z "$GLIDEPATH_BYTES" ]]; then
    GLIDEPATH_BYTES=$(awk -v c="$CONTRACT_BYTES" 'BEGIN { printf "%.0f", c * 1.5 }')
fi

# Calculate budget guardrail threshold (1.5x contract)
GUARDRAIL_BYTES=$(awk -v c="$CONTRACT_BYTES" 'BEGIN { printf "%.0f", c * 1.5 }')

# Build notifier array
if [[ -n "$NOTIFIER_ID" ]]; then
    NOTIFIER_JSON="[\"$NOTIFIER_ID\"]"
else
    NOTIFIER_JSON="[]"
fi

echo "=== Creating Cost Control Monitors ==="
echo "Deployment: $DEPLOYMENT"
echo "Audit dataset: $AUDIT_DATASET"
echo "Contract: $(format_bytes_rate "$CONTRACT_BYTES")"
echo "Budget guardrail: $(format_bytes_rate "$GUARDRAIL_BYTES") (1.5x)"
echo "Glidepath target: $(format_bytes_rate "$GLIDEPATH_BYTES")"
echo "Per-dataset spike: robust z-score > $ZSCORE_THRESHOLD, sustained $SPIKE_HOURS_THRESHOLD+ hours"
echo "Query cost spike: robust z-score > $ZSCORE_THRESHOLD, sustained $SPIKE_HOURS_THRESHOLD+ hours"
echo "Notifier: ${NOTIFIER_ID:-none}"
echo ""

# Check for existing monitors
echo "Checking for existing Cost Control monitors..."
EXISTING_IDS=$($AXIOM_API "$DEPLOYMENT" GET "/v2/monitors" 2>/dev/null | \
    jq -r '.[] | select(.name | startswith("Cost Control:")) | .id' 2>/dev/null || true)

if [[ -n "$EXISTING_IDS" ]]; then
    EXISTING_COUNT=$(echo "$EXISTING_IDS" | wc -l | tr -d ' ')
    
    if [[ "$FORCE" == "true" ]]; then
        echo "Found $EXISTING_COUNT existing monitors. Deleting (--force)..."
        echo "$EXISTING_IDS" | while read -r id; do
            echo "  Deleting $id..."
            $AXIOM_API "$DEPLOYMENT" DELETE "/v2/monitors/$id" >/dev/null 2>&1 || true
        done
        echo "Deleted. Proceeding with creation..."
        echo ""
    else
        EXISTING_NAMES=$($AXIOM_API "$DEPLOYMENT" GET "/v2/monitors" 2>/dev/null | \
            jq -r '.[] | select(.name | startswith("Cost Control:")) | .name' 2>/dev/null || true)
        echo ""
        echo "BLOCKER: MONITORS_ALREADY_EXIST"
        echo ""
        echo "Found $EXISTING_COUNT existing Cost Control monitors:"
        echo "$EXISTING_NAMES" | sed 's/^/  - /'
        echo ""
        echo "NEXT: scripts/create-monitors -d $DEPLOYMENT -a $AUDIT_DATASET -c $CONTRACT_BYTES --force"
        exit 1
    fi
else
    echo "No existing monitors found. Creating..."
    echo ""
fi

# =============================================================================
# MONITOR 1: BUDGET GUARDRAIL (Threshold)
# Immediate reaction: alerts when 24h ingest exceeds 1.5x contract
# =============================================================================

echo "1/4: Creating Budget Guardrail (threshold @ $(format_bytes "$GUARDRAIL_BYTES"))..."
$AXIOM_API "$DEPLOYMENT" POST "/v2/monitors" "$(cat <<EOF
{
  "name": "Cost Control: Budget Guardrail",
  "description": "Immediate alert when 24h ingest exceeds $(format_bytes_rate "$GUARDRAIL_BYTES") (1.5x contract of $(format_bytes_rate "$CONTRACT_BYTES")).",
  "type": "Threshold",
  "aplQuery": "['$AUDIT_DATASET']\n| where action == \"usageCalculated\"\n| where _time > ago(24h)\n| summarize total_bytes = sum(toreal(['properties.hourly_ingest_bytes']))",
  "operator": "AboveOrEqual",
  "threshold": $GUARDRAIL_BYTES,
  "intervalMinutes": 60,
  "rangeMinutes": 1440,
  "notifierIds": $NOTIFIER_JSON,
  "notifyByGroup": false,
  "triggerFromNRuns": 2
}
EOF
)"

echo ""

# =============================================================================
# MONITOR 2: PER-DATASET SPIKE (Robust z-score)
# Statistical attribution: detects which dataset's ingest pattern changed
# Uses log-transform + IQR-based sigma, dual gate (z>3 AND >p99), 2+ hours sustained
# =============================================================================

# Build the robust z-score APL query with rank-based materiality filtering
# Current window is 4h (not 7d) to avoid re-alerting on old spikes
# Rank-based: only alert on top N datasets by excess_bytes (scale-free, no hardcoded thresholds)
ZSCORE_QUERY="['$AUDIT_DATASET']
| where _time >= ago(4h) and _time < bin(now(), 1h) and action == \"usageCalculated\"
| extend bytes = toreal(['properties.hourly_ingest_bytes']), dataset = tostring(['properties.dataset'])
| where isfinite(bytes) and bytes >= 0
| summarize hourly_bytes = sum(bytes) by bucket = bin(_time, 1h), dataset
| extend hourly_y = log(hourly_bytes + 1)
| join kind=inner (
    ['$AUDIT_DATASET']
    | where _time >= ago(15d) and _time < ago(1h) and action == \"usageCalculated\"
    | extend bytes = toreal(['properties.hourly_ingest_bytes']), dataset = tostring(['properties.dataset'])
    | where isfinite(bytes) and bytes >= 0
    | summarize hourly_bytes = sum(bytes) by bin(_time, 1h), dataset
    | extend hourly_y = log(hourly_bytes + 1)
    | summarize baseline_hours = count(), y_p = percentiles_array(hourly_y, 25, 50, 75), b_p = percentiles_array(hourly_bytes, 50, 99) by dataset
    | where baseline_hours >= 72
    | extend median_y = todouble(y_p[1]), sigma_y = max_of((todouble(y_p[2]) - todouble(y_p[0])) / 1.349, 0.1), median_bytes = todouble(b_p[0]), p99_bytes = todouble(b_p[1])
) on dataset
| extend robust_z = (hourly_y - median_y) / sigma_y, excess_bytes = hourly_bytes - median_bytes
| where robust_z > $ZSCORE_THRESHOLD and hourly_bytes > p99_bytes and excess_bytes > 0
| summarize spike_hours = count(), max_z = round(max(robust_z), 2), max_excess_bytes = max(excess_bytes) by dataset
| where spike_hours >= $SPIKE_HOURS_THRESHOLD
| top 10 by max_excess_bytes desc
| summarize spike_count = count()"

# Escape for JSON
ZSCORE_QUERY_JSON=$(echo "$ZSCORE_QUERY" | jq -Rs '.')

echo "2/4: Creating Per-Dataset Spike Detection (robust z-score > $ZSCORE_THRESHOLD, $SPIKE_HOURS_THRESHOLD+ hours)..."
$AXIOM_API "$DEPLOYMENT" POST "/v2/monitors" "$(cat <<EOF
{
  "name": "Cost Control: Per-Dataset Spike",
  "description": "Robust z-score detection of dataset ingest spikes. Uses log-transform + IQR-based sigma. Alerts when z > $ZSCORE_THRESHOLD AND bytes > p99 for $SPIKE_HOURS_THRESHOLD+ hours.",
  "type": "Threshold",
  "aplQuery": $ZSCORE_QUERY_JSON,
  "operator": "AboveOrEqual",
  "threshold": 1,
  "intervalMinutes": 60,
  "rangeMinutes": 10080,
  "notifierIds": $NOTIFIER_JSON,
  "notifyByGroup": false,
  "triggerFromNRuns": 1
}
EOF
)"

echo ""

# =============================================================================
# MONITOR 3: QUERY COST SPIKE (Robust z-score, per-dataset)
# Statistical detection of query cost changes (different cost driver than ingest)
# Uses same robust z-score approach as ingest spike detection
# =============================================================================

# Build the robust z-score APL query for query costs with rank-based materiality filtering
# Current window is 4h (not 7d) to avoid re-alerting on old spikes
# Baseline is 15d to capture weekly seasonality patterns
# Rank-based: only alert on top N datasets by excess_gbms (scale-free, no hardcoded thresholds)
QCOST_ZSCORE_QUERY="['$AUDIT_DATASET']
| where _time >= ago(4h) and _time < bin(now(), 1h) and action == \"usageCalculated\"
| extend gbms = toreal(['properties.hourly_billable_query_gbms']), dataset = tostring(['properties.dataset'])
| where isfinite(gbms) and gbms >= 0
| summarize hourly_gbms = sum(gbms) by bucket = bin(_time, 1h), dataset
| extend hourly_y = log(hourly_gbms + 1)
| join kind=inner (
    ['$AUDIT_DATASET']
    | where _time >= ago(15d) and _time < ago(1h) and action == \"usageCalculated\"
    | extend gbms = toreal(['properties.hourly_billable_query_gbms']), dataset = tostring(['properties.dataset'])
    | where isfinite(gbms) and gbms >= 0
    | summarize hourly_gbms = sum(gbms) by bin(_time, 1h), dataset
    | extend hourly_y = log(hourly_gbms + 1)
    | summarize baseline_hours = count(), y_p = percentiles_array(hourly_y, 25, 50, 75), g_p = percentiles_array(hourly_gbms, 50, 99) by dataset
    | where baseline_hours >= 72
    | extend median_y = todouble(y_p[1]), sigma_y = max_of((todouble(y_p[2]) - todouble(y_p[0])) / 1.349, 0.1), median_gbms = todouble(g_p[0]), p99_gbms = todouble(g_p[1])
) on dataset
| extend robust_z = (hourly_y - median_y) / sigma_y, excess_gbms = hourly_gbms - median_gbms
| where robust_z > $ZSCORE_THRESHOLD and hourly_gbms > p99_gbms and excess_gbms > 0
| summarize spike_hours = count(), max_z = round(max(robust_z), 2), max_excess_gbms = max(excess_gbms) by dataset
| where spike_hours >= $SPIKE_HOURS_THRESHOLD
| top 10 by max_excess_gbms desc
| summarize spike_count = count()"

# Escape for JSON
QCOST_ZSCORE_QUERY_JSON=$(echo "$QCOST_ZSCORE_QUERY" | jq -Rs '.')

echo "3/4: Creating Query Cost Spike Detection (robust z-score > $ZSCORE_THRESHOLD, $SPIKE_HOURS_THRESHOLD+ hours)..."
$AXIOM_API "$DEPLOYMENT" POST "/v2/monitors" "$(cat <<EOF
{
  "name": "Cost Control: Query Cost Spike",
  "description": "Robust z-score detection of per-dataset query cost spikes. Uses log-transform + IQR-based sigma. Alerts when z > $ZSCORE_THRESHOLD AND gbms > p99 for $SPIKE_HOURS_THRESHOLD+ hours.",
  "type": "Threshold",
  "aplQuery": $QCOST_ZSCORE_QUERY_JSON,
  "operator": "AboveOrEqual",
  "threshold": 1,
  "intervalMinutes": 60,
  "rangeMinutes": 10080,
  "notifierIds": $NOTIFIER_JSON,
  "notifyByGroup": false,
  "triggerFromNRuns": 1
}
EOF
)"

echo ""

# =============================================================================
# MONITOR 4: REDUCTION GLIDEPATH (Threshold)
# Progress tracking: alerts when daily ingest exceeds reduction target
# =============================================================================

echo "4/4: Creating Reduction Glidepath (threshold @ $(format_bytes "$GLIDEPATH_BYTES"))..."
$AXIOM_API "$DEPLOYMENT" POST "/v2/monitors" "$(cat <<EOF
{
  "name": "Cost Control: Reduction Glidepath",
  "description": "Tracks progress toward $(format_bytes_rate "$CONTRACT_BYTES") contract. Current target: $(format_bytes_rate "$GLIDEPATH_BYTES"). Update with update-glidepath script.",
  "type": "Threshold",
  "aplQuery": "['$AUDIT_DATASET']\n| where action == \"usageCalculated\"\n| where _time > ago(24h)\n| summarize daily_ingest = sum(toreal(['properties.hourly_ingest_bytes']))",
  "operator": "AboveOrEqual",
  "threshold": $GLIDEPATH_BYTES,
  "intervalMinutes": 360,
  "rangeMinutes": 1440,
  "notifierIds": $NOTIFIER_JSON,
  "notifyByGroup": false,
  "triggerFromNRuns": 1
}
EOF
)"

echo ""
echo "=============================================="
echo "Done! Created 4 cost control monitors."
echo "=============================================="
echo ""
echo "IMMEDIATE REACTION:"
echo "  1. Budget Guardrail (threshold @ $(format_bytes "$GUARDRAIL_BYTES"))"
echo "     Alerts within 1 hour if total ingest exceeds limit"
echo ""
echo "STATISTICAL ATTRIBUTION:"
echo "  2. Per-Dataset Ingest Spike (robust z-score > $ZSCORE_THRESHOLD, $SPIKE_HOURS_THRESHOLD+ hours)"
echo "     Log-transform + IQR-based sigma; dual gate (z > 3 AND bytes > p99)"
echo "  3. Per-Dataset Query Cost Spike (robust z-score > $ZSCORE_THRESHOLD, $SPIKE_HOURS_THRESHOLD+ hours)"
echo "     Same approach for query costs (GB·ms)"
echo ""
echo "PROGRESS TRACKING:"
echo "  4. Reduction Glidepath (threshold @ $(format_bytes "$GLIDEPATH_BYTES"))"
echo "     Update weekly: scripts/update-glidepath -d $DEPLOYMENT -t <new-target>"
echo ""

# Get org_id for URL (best-effort)
CONFIG_FILE="$HOME/.axiom.toml"
if [[ -f "$CONFIG_FILE" ]]; then
    ORG_ID=$(awk -v deployment="$DEPLOYMENT" '
        /^\[deployments\./ { in_deployment = ($0 ~ "\\[deployments\\." deployment "\\]") }
        in_deployment && $1 == "org_id" { gsub(/[" ]/, "", $3); print $3; exit }
    ' "$CONFIG_FILE" || true)
    if [[ -n "${ORG_ID:-}" ]]; then
        echo "View at: https://app.axiom.co/${ORG_ID}/monitors"
    fi
fi
